services:
  example-pytorch:
    build:
      context: .
      dockerfile: ../../pytorch/Dockerfile-cpu
    image: example-pytorch:latest
    container_name: example-pytorch
    ports:
      - "8888:8888"
    volumes:
      - ../../../:/example-pytorch
      - ../../../mlflow-artifacts/mlruns:/mlruns
    shm_size: "8gb"           # 大きめの共有メモリ（DataLoaderなどで有効）
    environment:
      - TZ=Asia/Tokyo
      - NVIDIA_VISIBLE_DEVICES=all
      - PYTHONUNBUFFERED=1
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    restart: unless-stopped
    tty: true
    stdin_open: true
    command: > 
      jupyter lab --ip=0.0.0.0
                  --allow-root
                  --LabApp.token=''

  mlflow:
    build:
      context: .
      dockerfile: ../../mlflow/Dockerfile
    image: example-mlflow:latest
    container_name: example-pytorch-mlflow
    ports:
      - "5000:5000"
    volumes:
      - ../../../mlflow-artifacts/mlruns:/mlruns
    environment:
      - TZ=Asia/Tokyo
    restart: unless-stopped
    tty: true
    command: >
      mlflow server --host 0.0.0.0
                    --port 5000
                    --backend-store-uri sqlite:////mlruns/mlflow.db
                    --default-artifact-root file:///mlruns/artifacts

  tensorboard:
    image: tensorflow/tensorflow:latest
    container_name: example-pytorch-tensorboard
    ports:
      - "6006:6006"
    volumes:
      - ../../../logs:/logs
    environment:
      - TZ=Asia/Tokyo
    restart: unless-stopped
    command: >
      tensorboard --logdir=/logs
                  --host=0.0.0.0
                  --port=6006
